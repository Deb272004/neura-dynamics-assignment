{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "question_type = [\n",
    "    \"Answerable\",\"Answerable\",\"Answerable\",\"Partially Answerable\",\"Partially Answerable\",\"Unanswerable\",\"Unanswerable\",\"Answerable\"\n",
    "]\n",
    "inputs = [\n",
    "    \"What are the seven minimum items that should be included in an employee handbook according to this template?\",\n",
    "    \"Under what conditions is an employee eligible for leave under the federal Family and Medical Leave Act (FMLA)?\",\n",
    "    \"What is the company's policy regarding the use of \\\"pirated\\\" software or installing new software on company computers?\",\n",
    "    \"How many days of sick leave are employees granted each year, and are they paid or unpaid?\",\n",
    "    \"What specific holidays does the company observe, and are employees paid for these days?\",\n",
    "    \"What is the specific dollar amount of the replacement fee if an employee loses their paycheck?\",\n",
    "    \"Does the company provide a \\\"Childcare Subsidy\\\" or \\\"Commuter Benefits\\\" for full-time employees?\",\n",
    "    \"What is the policy on pirated software?\"\n",
    "\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"According to the provided template, the seven minimum items that should be included in an employee handbook are:1. **Employment at-will disclaimer (section 1.3)**: A statement indicating that employment is at-will, meaning either the employee or the company may terminate employment at any time, with or without reason.2. **Equal employment opportunity statement (section 2.1)**: A statement regarding equal employment opportunity, emphasizing the company's commitment to providing equal opportunities for all employees.3. **Policy prohibiting unlawful discrimination and harassment (section 2.2)**: A policy that prohibits unlawful discrimination and harassment in the workplace.4. **Section on company property and privacy rules (section 3)**: A section that describes the policy for use of company property and privacy rules, including expectations for employee behavior and consequences for misuse.5. **Section on employment classification and overtime rules**: A section that explains the company's employment classification system (e.g., exempt vs. non-exempt) and overtime rules.6. **A policy on Family and Medical Leave (section 6.3)**: A policy that outlines the company's Family and Medical Leave (FMLA) policy, including eligibility, leave procedures, and return-to-work requirements (applicable to companies with 50 or more employees).7. **A section on Safety (section 9)**: A section that outlines the company's safety policies and procedures, including emergency protocols and employee responsibilities.\",\n",
    "    \"According to the provided template, an employee is eligible for leave under the federal Family and Medical Leave Act (FMLA) if they have worked as an employee of the company for at least 1,250 hours for twelve months. This eligibility criterion is a requirement for FMLA leave.Additionally, the employee must be taking leave for one of the following reasons:1. The birth of a child and to care for the newborn child;2. Placement of a child into adoptive or foster care with the employee;3. Care for a spouse, son, daughter, or parent who has a serious health condition; or4. Care for the employee's own serious health condition.It's worth noting that the company may require employees to exhaust their accrued paid leave at the same time they are on FMLA leave, if the company has 50 or more employees.\",\n",
    "    \"According to the provided context, the company has the following policy regarding the use of \\\"pirated\\\" software or installing new software on company computers:* Employees are prohibited from downloading any \\\"pirated\\\" software, files, or programs.* Employees must receive permission from a supervisor before installing any new software on a company computer.* Files or programs stored on company computers may not be copied for personal use.This policy is in place to protect the company's computer systems and data from potential security risks and to ensure that all software used on company computers is legitimate and authorized.\",\n",
    "    \"The document states that sick days are granted to regular employees, but the specific number of days is not defined and must be filled in by the employer. Similarly, the handbook provides a placeholder to designate whether these days are paid or unpaid.\",\n",
    "    \"The company observes New Year's Day, Martin Luther King, Jr. Day, Memorial Day, Independence Day, Labor Day, Thanksgiving, and Christmas Day. However, whether these are paid or unpaid depends on the specific employer's selection in the template.\",\n",
    "    \"The document states that the company reserves the right to charge a replacement fee for lost paychecks,  but it does not specify the actual cost or dollar amount of that fee.\",\n",
    "    \"The provided document does not mention childcare subsidies or commuter benefits. It lists general benefits like health insurance, retirement plans, and workers' compensation.\",\n",
    "    \"According to Section 3.4, employees are not permitted to download any \\\"pirated\\\" software, files, or programs and must receive permission from a supervisor before installing any new software. Additionally, employees should have no expectation of privacy when using company computers.\",\n",
    "]\n",
    "\n",
    "# Dataset\n",
    "qa_pairs = [{\"question_type\":qt,\"question\": q, \"answer\": a} for qt, q, a in zip(question_type ,inputs, outputs)]\n",
    "df = pd.DataFrame(qa_pairs)\n",
    "\n",
    "# Write to csv\n",
    "csv_path = \"/home/deblina/Documents/projects/neura dynamics assignment/data/goldens.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['c63e86f8-514e-4b97-bef9-7ff3e667774b',\n",
       "  '56827991-ef56-49ae-9e39-21c0357d5872',\n",
       "  'a622cb02-8345-4191-8a0b-51e53ccb96a8',\n",
       "  'd8dc3f76-5efd-4bf8-b1d6-24e95f50d4b0',\n",
       "  'c3da2263-adb0-48cb-9126-b81f3c802095',\n",
       "  'acd74396-c871-455e-9dd3-ce87e25b30ae',\n",
       "  '532e55d2-ede3-4b72-83b0-d101786abe94',\n",
       "  'f77afa60-dd6c-4c5c-9fa4-05f6dced4ce0'],\n",
       " 'count': 8,\n",
       " 'as_of': '2026-02-07T06:37:42.669516812Z'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "client = Client()\n",
    "\n",
    "dataset_name = \"company-policy-goldens\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Input and expected output pair for Company Policy\"\n",
    ")\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\":q} for q in inputs],\n",
    "    outputs=[{\"answer\":a} for a in outputs],\n",
    "    dataset_id=dataset.id\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/deblina/Documents/projects/neura dynamics assignment\")\n",
    "from pathlib import Path\n",
    "from company_policy_chat.src.document_ingestion.ingestion import Ingestion\n",
    "from company_policy_chat.src.document_retrieval.retrieval import Retrieval\n",
    "\n",
    "\n",
    "\n",
    "class LocalFileAdapter:\n",
    "    def __init__(self, file_path: str):\n",
    "        self.path = Path(file_path)\n",
    "        self.filename = self.path.name # Matches UploadFile property\n",
    "    \n",
    "    def read(self, size=-1):\n",
    "        \"\"\"Standard file read method.\"\"\"\n",
    "        return self.path.read_bytes()\n",
    "\n",
    "    # Optional: Add seek for compatibility\n",
    "    def seek(self, offset, whence=0):\n",
    "        pass\n",
    "\n",
    "\n",
    "def answer_ai_report_question(\n",
    "    inputs: dict,\n",
    "    data_path: str = \"/home/deblina/Documents/projects/neura dynamics assignment/data/Small_Business_Administration_Employee_polich_Template.pdf\",\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    ") -> dict:\n",
    "    \n",
    "    try:\n",
    "        # Extract question from inputs\n",
    "        question = inputs.get(\"question\", \"\")\n",
    "        if not question:\n",
    "            return {\"answer\": \"No question provided\"}\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not Path(data_path).exists():\n",
    "            return {\"answer\": f\"Data file not found: {data_path}\"}\n",
    "        \n",
    "        # Create file adapter\n",
    "        file_adapter = LocalFileAdapter(data_path)\n",
    "        \n",
    "        # Build index using ChatIngestor\n",
    "        ingestor = Ingestion(\n",
    "            temp_base=\"/home/deblina/Documents/projects/neura dynamics assignment/data\",\n",
    "            faiss_base=\"/home/deblina/Documents/projects/neura dynamics assignment/faiss_index\",\n",
    "        )\n",
    "        \n",
    "        # Build retriever\n",
    "        ingestor.build_index(\n",
    "            uploaded_files=[file_adapter],\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "        \n",
    "        index_path = \"/home/deblina/Documents/projects/neura dynamics assignment/faiss_index\"\n",
    "        \n",
    "        # Create RAG instance and load retriever\n",
    "        rag = Retrieval()\n",
    "        rag.load_retriever_from_faiss(\n",
    "            index_path=index_path,\n",
    "            index_name=\"index\"\n",
    "        )\n",
    "        \n",
    "        # Get answer\n",
    "        answer = rag.invoke(question, chat_history=[])\n",
    "        \n",
    "        return {\"answer\": answer}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"answer\": f\"Error: {str(e)}\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2026-02-07 12:07:42\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mIngestion initialized         \u001b[0m \u001b[36mfaiss_dir\u001b[0m=\u001b[35m'/home/deblina/Documents/projects/neura dynamics assignment/faiss_index'\u001b[0m \u001b[36mtemp_dir\u001b[0m=\u001b[35m'/home/deblina/Documents/projects/neura dynamics assignment/data'\u001b[0m\n",
      "\u001b[2m2026-02-07 12:07:42\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mFile saved                    \u001b[0m \u001b[36mfile\u001b[0m=\u001b[35mSmall_Business_Administration_Employee_polich_Template.pdf\u001b[0m \u001b[36msize\u001b[0m=\u001b[35m3625853\u001b[0m\n",
      "\u001b[2m2026-02-07 12:07:43\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDocuments loaded              \u001b[0m \u001b[36mcount\u001b[0m=\u001b[35m35\u001b[0m\n",
      "\u001b[2m2026-02-07 12:07:43\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDocuments split into chunks   \u001b[0m \u001b[36mchunk_overlap\u001b[0m=\u001b[35m200\u001b[0m \u001b[36mchunk_size\u001b[0m=\u001b[35m1000\u001b[0m \u001b[36mchunks\u001b[0m=\u001b[35m79\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading embedding model: BBAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2026-02-07 12:07:43\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mLoading existing FAISS index  \u001b[0m \u001b[36mpath\u001b[0m=\u001b[35m'/home/deblina/Documents/projects/neura dynamics assignment/faiss_index'\u001b[0m\n",
      "\u001b[2m2026-02-07 12:07:43\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo new documents to add — index is up to date\u001b[0m\n",
      "\u001b[2m2026-02-07 12:07:43\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mFAISS index ready             \u001b[0m \u001b[36mindex_path\u001b[0m=\u001b[35m'/home/deblina/Documents/projects/neura dynamics assignment/faiss_index'\u001b[0m \u001b[36mtotal_chunks\u001b[0m=\u001b[35m79\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing LLM: groq | Model: llama-3.1-8b-instant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2026-02-07 12:07:43\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mLLM loaded successfully       \u001b[0m\n",
      "\u001b[2m2026-02-07 12:07:43\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mRetrieval class initialized successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading embedding model: BBAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2026-02-07 12:07:43\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mLCEL chain built successfully \u001b[0m\n",
      "\u001b[2m2026-02-07 12:07:43\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mFAISS retriever loaded successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Does the company follow a strict \"progressive discipline\" system (e.g., verbal warning, then written, then suspension) for all employee misconduct?\n",
      "\n",
      "Answer: According to Section 8.2 of the Discipline Policy, the company may use a progressive discipline system, but it is not mandatory. The policy states that discipline may take the form of \"oral warnings, written warnings, probation, suspension, demotion, discharge, removal or some other disciplinary action, in no particular order.\" This suggests that the company reserves the right to determine the course of action on a case-by-case basis, rather than strictly adhering to a progressive discipline system.\n",
      "\n",
      "However, the company may choose to use a progressive discipline system in certain situations, as indicated by the mention of it in the policy. If you have specific questions about the company's approach to discipline, I recommend consulting with your supervisor or HR representative for more information.\n",
      "\n",
      "Please note that the company may revise or update its policies at any time, so it's essential to stay informed about any changes to the discipline policy or procedures.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Test the function with a sample question\n",
    "test_input = {\"question\": \"Does the company follow a strict \\\"progressive discipline\\\" system (e.g., verbal warning, then written, then suspension) for all employee misconduct?\"}\n",
    "result = answer_ai_report_question(test_input)\n",
    "print(\"Question:\", test_input[\"question\"])\n",
    "print(\"\\nAnswer:\", result[\"answer\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing LLM: groq | Model: llama-3.1-8b-instant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2026-02-07 12:11:16\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mLLM loaded successfully       \u001b[0m\n",
      "\u001b[2m2026-02-07 12:11:16\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mRetrieval class initialized successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading embedding model: BBAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2026-02-07 12:11:16\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mLCEL chain built successfully \u001b[0m\n",
      "\u001b[2m2026-02-07 12:11:16\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mFAISS retriever loaded successfully\u001b[0m\n",
      "View the evaluation results for experiment: 'company-policy-rag-eval-96e150b4' at:\n",
      "https://smith.langchain.com/o/99040f25-cf72-4aea-9165-21a9a528a5b7/datasets/ecb2b661-b036-4472-a7ea-e91800deebc6/compare?selectedSessions=d3d1c57b-55a7-453c-8f53-eb8cb9ad2f6f\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score extracted: 1 from 'Score: 1\n",
      "\n",
      "The output is factually correct and complete compared to the reference. It accurately conv...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "1it [00:01,  1.38s/it]HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2it [00:01,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score extracted: 1 from 'Score: 1\n",
      "\n",
      "The output is factually correct and complete compared to the reference. It accurately stat...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "3it [00:02,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score extracted: 1 from 'Score: 1\n",
      "\n",
      "The output accurately reflects the information provided in the reference, stating that the...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 3.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 9.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score extracted: 1 from 'Score: 1\n",
      "\n",
      "The output is factually correct and complete because it accurately states that the company...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 3.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "4it [00:15,  5.42s/it]HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 3.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 9.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 1.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score extracted: 1 from 'Score: 1\n",
      "\n",
      "The output accurately reflects the information provided in the reference, stating that the...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "5it [00:28,  8.41s/it]HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 4.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 9.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 1.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score extracted: 0 from 'Score: 0\n",
      "\n",
      "The output is missing the eligibility criteria for FMLA leave, which is not just about wor...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 4.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "6it [00:44, 10.91s/it]HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 1.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 9.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score extracted: 1 from 'Score: 1\n",
      "\n",
      "The output is factually correct as it lists the same holidays as the reference. However, i...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 3.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "7it [00:58, 12.01s/it]HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 8.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "8it [01:07,  8.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score extracted: 0 from 'Score: 0\n",
      "\n",
      "The output is not factually correct and complete compared to the reference. The main discr...'\n",
      "✅ Evaluation completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os  \n",
    "\n",
    "groq_api_key = os.environ.get('GROQ_API_KEY')\n",
    "if not groq_api_key:\n",
    "    raise ValueError(\"GROQ_API_KEY environment variable is not set. Please set it before running the script.\")\n",
    "\n",
    "from langsmith import Client\n",
    "from langchain_groq import ChatGroq  \n",
    "import re  \n",
    "import time  \n",
    "\n",
    "\n",
    "client = Client()\n",
    "\n",
    "from company_policy_chat.src.document_retrieval.retrieval import Retrieval\n",
    "\n",
    "rag = Retrieval()\n",
    "index_path = \"/home/deblina/Documents/projects/neura dynamics assignment/faiss_index\"\n",
    "\n",
    "rag.load_retriever_from_faiss(\n",
    "    index_path=index_path,\n",
    "    index_name=\"index\"\n",
    ")\n",
    "\n",
    "\n",
    "def target(inputs: dict) -> dict:\n",
    "    \n",
    "    answer = rag.invoke(inputs[\"question\"], chat_history=[])\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "\n",
    "def correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "    \n",
    "    llm = ChatGroq(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        api_key=groq_api_key,\n",
    "        temperature=0,  \n",
    "    )\n",
    "    \n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Evaluate the correctness of the following output based on the reference.\n",
    "\n",
    "    Question: {inputs['question']}\n",
    "    Output: {outputs['answer']}\n",
    "    Reference: {reference_outputs.get('answer', '')}\n",
    "\n",
    "    Is the output factually correct and complete compared to the reference? Respond with only: \"Score: 1\" if yes, or \"Score: 0\" if no. Include a brief reasoning if needed, but end with the score.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_retries = 5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            response_text = response.content.strip()\n",
    "            \n",
    "            # Parse score from response\n",
    "            match = re.search(r\"Score:\\s*([01])\", response_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                score = int(match.group(1))\n",
    "                print(f\"Score extracted: {score} from '{response_text[:100]}...'\")\n",
    "                return {\"key\": \"correctness\", \"score\": score}\n",
    "            else:\n",
    "                \n",
    "                if re.search(r\"(?i)(yes|true|correct)\", response_text):\n",
    "                    score = 1\n",
    "                elif re.search(r\"(?i)(no|false|incorrect)\", response_text):\n",
    "                    score = 0\n",
    "                else:\n",
    "                    score = 0  \n",
    "                print(f\"Fallback score: {score} from '{response_text[:100]}...'\")\n",
    "                return {\"key\": \"correctness\", \"score\": score}\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluator attempt {attempt + 1} failed: {e}\")\n",
    "            if \"429\" in str(e) or \"rate limit\" in str(e).lower():\n",
    "                sleep_time = 2 ** attempt * 3  \n",
    "                print(f\"Rate limited. Sleeping for {sleep_time}s...\")\n",
    "                time.sleep(sleep_time)\n",
    "            elif attempt < max_retries - 1:\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                print(\"Max retries reached. Defaulting score to 0.\")\n",
    "                return {\"key\": \"correctness\", \"score\": 0}\n",
    "\n",
    "\n",
    "try:\n",
    "    experiment_results = client.evaluate(\n",
    "        target,\n",
    "        data=\"company-policy-goldens\",  \n",
    "        evaluators=[correctness_evaluator],\n",
    "        experiment_prefix=\"company-policy-rag-eval\",\n",
    "        max_concurrency=1,  \n",
    "    )\n",
    "\n",
    "    print(\"✅ Evaluation completed successfully!\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"❌ Evaluation interrupted (e.g., due to rate limits or manual stop). Check partial results in LangSmith.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Evaluation failed with error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
